WARNING:root:Pytorch pre-release version 1.12.0a0+gita4ad332 - assuming intent to test it
Falling back to Fairscale checkpoint
WARNING:root:Pytorch pre-release version 1.12.0a0+gita4ad332 - assuming intent to test it
WARNING:root:Pytorch pre-release version 1.12.0a0+gita4ad332 - assuming intent to test it
WARNING:root:Pytorch pre-release version 1.12.0a0+gita4ad332 - assuming intent to test it
WARNING:root:Pytorch pre-release version 1.12.0a0+gita4ad332 - assuming intent to test it
WARNING:root:Pytorch pre-release version 1.12.0a0+gita4ad332 - assuming intent to test it
WARNING:root:Pytorch pre-release version 1.12.0a0+gita4ad332 - assuming intent to test it
WARNING:root:Pytorch pre-release version 1.12.0a0+gita4ad332 - assuming intent to test it
WARNING:root:Pytorch pre-release version 1.12.0a0+gita4ad332 - assuming intent to test it
Falling back to Fairscale checkpoint
Recursing for <class 'models.EmbeddingStem'>
Recursing for <class 'torch.nn.modules.sparse.Embedding'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
number of parameters: 1939387536
Recursing for <class 'models.ShardedGPT'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.EmbeddingStem'>
Recursing for <class 'torch.nn.modules.sparse.Embedding'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
deferred build time: 1.185296142578125 sec 0.0040592333649935785 per fsdp instance, 292 instances
 -- nothing is fake --
Initialized both FSDP models
Falling back to Fairscale checkpoint
Recursing for <class 'models.EmbeddingStem'>
Recursing for <class 'torch.nn.modules.sparse.Embedding'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
number of parameters: 1939387536
Recursing for <class 'models.ShardedGPT'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.EmbeddingStem'>
Recursing for <class 'torch.nn.modules.sparse.Embedding'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
 -- nothing is fake --
Initialized both FSDP models
Falling back to Fairscale checkpoint
Recursing for <class 'models.EmbeddingStem'>
Recursing for <class 'torch.nn.modules.sparse.Embedding'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
number of parameters: 1939387536
Recursing for <class 'models.ShardedGPT'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.EmbeddingStem'>
Recursing for <class 'torch.nn.modules.sparse.Embedding'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
 -- nothing is fake --
Initialized both FSDP models
Falling back to Fairscale checkpoint
Recursing for <class 'models.EmbeddingStem'>
Recursing for <class 'torch.nn.modules.sparse.Embedding'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
number of parameters: 1939387536
Recursing for <class 'models.ShardedGPT'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.EmbeddingStem'>
Recursing for <class 'torch.nn.modules.sparse.Embedding'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
 -- nothing is fake --
Initialized both FSDP models
Falling back to Fairscale checkpoint
Recursing for <class 'models.EmbeddingStem'>
Recursing for <class 'torch.nn.modules.sparse.Embedding'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
number of parameters: 1939387536
Recursing for <class 'models.ShardedGPT'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.EmbeddingStem'>
Recursing for <class 'torch.nn.modules.sparse.Embedding'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
 -- nothing is fake --
Initialized both FSDP models
Falling back to Fairscale checkpoint
Recursing for <class 'models.EmbeddingStem'>
Recursing for <class 'torch.nn.modules.sparse.Embedding'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
number of parameters: 1939387536
Recursing for <class 'models.ShardedGPT'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.EmbeddingStem'>
Recursing for <class 'torch.nn.modules.sparse.Embedding'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
 -- nothing is fake --
Initialized both FSDP models
Falling back to Fairscale checkpoint
Recursing for <class 'models.EmbeddingStem'>
Recursing for <class 'torch.nn.modules.sparse.Embedding'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
number of parameters: 1939387536
Recursing for <class 'models.ShardedGPT'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.EmbeddingStem'>
Recursing for <class 'torch.nn.modules.sparse.Embedding'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
 -- nothing is fake --
Initialized both FSDP models
Falling back to Fairscale checkpoint
Recursing for <class 'models.EmbeddingStem'>
Recursing for <class 'torch.nn.modules.sparse.Embedding'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
number of parameters: 1939387536
Recursing for <class 'models.ShardedGPT'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.EmbeddingStem'>
Recursing for <class 'torch.nn.modules.sparse.Embedding'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.Block'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'models.CausalSelfAttention'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.container.Sequential'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.activation.GELU'>
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.nn.modules.dropout.Dropout'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.normalization.LayerNorm'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
Recursing for <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
Recursing for <class 'torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper'>
Recursing for <class 'torch.nn.modules.linear.Linear'>
<class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'> did NOT pass check
 -- nothing is fake --
Initialized both FSDP models
