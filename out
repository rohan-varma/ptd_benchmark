WARNING:root:Pytorch pre-release version 1.12.0a0+gita4ad332 - assuming intent to test it
Falling back to Fairscale checkpoint
WARNING:root:Pytorch pre-release version 1.12.0a0+gita4ad332 - assuming intent to test it
WARNING:root:Pytorch pre-release version 1.12.0a0+gita4ad332 - assuming intent to test it
WARNING:root:Pytorch pre-release version 1.12.0a0+gita4ad332 - assuming intent to test it
WARNING:root:Pytorch pre-release version 1.12.0a0+gita4ad332 - assuming intent to test it
WARNING:root:Pytorch pre-release version 1.12.0a0+gita4ad332 - assuming intent to test it
WARNING:root:Pytorch pre-release version 1.12.0a0+gita4ad332 - assuming intent to test it
WARNING:root:Pytorch pre-release version 1.12.0a0+gita4ad332 - assuming intent to test it
WARNING:root:Pytorch pre-release version 1.12.0a0+gita4ad332 - assuming intent to test it
Falling back to Fairscale checkpoint
number of parameters: 6469853184
n_wrap 196
Falling back to Fairscale checkpoint
number of parameters: 6469853184
n_wrap 196
Wrapping 6469853184
Wrapping 13107200
Successfully wrapped 13107200 params
Wrapping 8192
Successfully wrapped 8192 params
Wrapping 12582912
Successfully wrapped 12582912 params
Falling back to Fairscale checkpoint
number of parameters: 6469853184
n_wrap 196
Falling back to Fairscale checkpoint
number of parameters: 6469853184
n_wrap 196
Falling back to Fairscale checkpoint
number of parameters: 6469853184
n_wrap 196
Falling back to Fairscale checkpoint
number of parameters: 6469853184
n_wrap 196
Falling back to Fairscale checkpoint
number of parameters: 6469853184
n_wrap 196
Falling back to Fairscale checkpoint
number of parameters: 6469853184
n_wrap 196
Traceback (most recent call last):
  File "meta_test.py", line 212, in <module>
    mp.spawn(worker, nprocs=8, args=())
  File "/fsx/users/rvarm1/rvarm1/repos/pytorch/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/fsx/users/rvarm1/rvarm1/repos/pytorch/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/fsx/users/rvarm1/rvarm1/repos/pytorch/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 6 terminated with the following error:
Traceback (most recent call last):
  File "/fsx/users/rvarm1/rvarm1/repos/pytorch/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 319, in __init__
    self._fsdp_wrapped_module: FlattenParamsWrapper = FlattenParamsWrapper(
  File "/fsx/users/rvarm1/rvarm1/repos/pytorch/torch/distributed/fsdp/flatten_params_wrapper.py", line 294, in __init__
    self.flat_param = FlatParameter(params, params[0].requires_grad)
  File "/fsx/users/rvarm1/rvarm1/repos/pytorch/torch/distributed/fsdp/flatten_params_wrapper.py", line 132, in __new__
    data = torch.cat(
RuntimeError: CUDA out of memory. Tried to allocate 24.01 GiB (GPU 6; 31.75 GiB total capacity; 24.02 GiB already allocated; 5.95 GiB free; 24.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/fsx/users/rvarm1/rvarm1/repos/pytorch/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/fsx/users/rvarm1/rvarm1/repos/ptd_benchmark/meta_test.py", line 204, in worker
    run_test_with_deferred(d, r)
  File "/fsx/users/rvarm1/rvarm1/repos/ptd_benchmark/meta_test.py", line 142, in run_test_with_deferred
    run_deferred()
  File "/fsx/users/rvarm1/rvarm1/repos/ptd_benchmark/meta_test.py", line 131, in run_deferred
    fsdp_model = FSDP(model, fsdp_auto_wrap_policy=pol, param_init_fns=init_fn)
  File "/fsx/users/rvarm1/rvarm1/repos/pytorch/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 324, in __init__
    raise ValueError(f"Error when wrapping {num_params} params: {str(e)}")
ValueError: Error when wrapping 6444154880 params: CUDA out of memory. Tried to allocate 24.01 GiB (GPU 6; 31.75 GiB total capacity; 24.02 GiB already allocated; 5.95 GiB free; 24.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

